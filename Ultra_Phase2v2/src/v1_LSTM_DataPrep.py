

# from six.moves import xrange
import six.moves.cPickle as pickle

import gzip
import os
import numpy
import theano


'''
####################################################################################
'''

def prepare_data(seqs, labels, maxlen=None):
    """
    Create the matrices from the datasets.

    This pad each sequence to the same lenght: 
    the lenght of the longuest sequence or maxlen.
    if maxlen is set, we will cut all sequence to this maximum lenght.

    This swap the axis!

    """
    # seqs or x: a list of sentences
    lengths = [len(s) for s in seqs]
    # use maxlen to truncate data
    if maxlen is not None:
        new_seqs = []
        new_labels = []
        new_lengths = []
        for l, s, y in zip(lengths, seqs, labels):
            if l < maxlen:
                new_seqs.append(s) # X
                new_labels.append(y) # Y
                new_lengths.append(l)
        lengths = new_lengths
        labels = new_labels # Y
        seqs = new_seqs # X

        if len(lengths) < 1:
            return None, None, None

    n_samples = len(seqs) # X
    maxlen = numpy.max(lengths)
    # X matrix, maxlen rows, n_samples columns
    # each sentence is on a column
    x = numpy.zeros((maxlen, n_samples)).astype('int64')
    x_mask = numpy.zeros((maxlen, n_samples)).astype(theano.config.floatX)
    # load data into X, data as values of each word from dict of word2index
    # mark x_mask with '1' on cells with words, '0' on cells without
    for idx, s in enumerate(seqs):
        x[:lengths[idx], idx] = s
        x_mask[:lengths[idx], idx] = 1.

    ####################################################################################
    return x, x_mask, labels


'''
####################################################################################
'''

def load_data(dataset_name, path_dataset, 
              n_words=60000, valid_portion=0.1, maxlen=None,
              sort_by_len=False):
    '''
    Loads the dataset
    
    dataset_name: e.g. 'FullScoredTweets_train.pkl'
    path_dataset: e.g. '../Data/DataSet_Training/'
    '''
    #################################################################
    # Load the dataset

    root_path = os.path.dirname(os.path.realpath('__file__'))
    dataset = os.path.join(root_path, path_dataset)
    dataset = dataset + dataset_name
    
    f = open(dataset, 'rb')
    # training data set is generated by 2 pkl.dump()
    train_set = pickle.load(f) # train_set is tuple, (X, Y)
    test_set = pickle.load(f) 

    f.close()
    #################################################################   

    # limit sentence length by maxlen
    if maxlen:
        new_train_set_x = []
        new_train_set_y = []
        for x, y in zip(train_set[0], train_set[1]):
            if len(x) < maxlen:
                new_train_set_x.append(x)
                new_train_set_y.append(y)
        train_set = (new_train_set_x, new_train_set_y)
        del new_train_set_x, new_train_set_y

    # split training set into validation set
    train_set_x, train_set_y = train_set
    n_samples = len(train_set_x)
    sidx = numpy.random.permutation(n_samples) # draw validation set randomly
    n_train = int(numpy.round(n_samples * (1. - valid_portion)))
    
    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]
    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]
    
    train_set_x = [train_set_x[s] for s in sidx[:n_train]]
    train_set_y = [train_set_y[s] for s in sidx[:n_train]]

    train_set = (train_set_x, train_set_y)
    valid_set = (valid_set_x, valid_set_y)

    #################################################################
    def remove_unk(x):
        # flag words by whether their frequency is above the threshold or not
        # threshod set by n_words and the dict() used to word2index 
        return [ [1 if w >= n_words else w for w in sen] for sen in x ]
    #################################################################

    test_set_x, test_set_y = test_set
    valid_set_x, valid_set_y = valid_set
    train_set_x, train_set_y = train_set

    train_set_x = remove_unk(train_set_x)
    valid_set_x = remove_unk(valid_set_x)
    test_set_x = remove_unk(test_set_x)

    #################################################################
    def len_argsort(seq):
        # seq: list of lists
        # sort seq by its elements length
        return sorted( range( len(seq) ), key=lambda x: len(seq[x])
                     )
    #################################################################

    if sort_by_len:
        sorted_index = len_argsort(test_set_x)
        test_set_x = [test_set_x[i] for i in sorted_index]
        test_set_y = [test_set_y[i] for i in sorted_index]

        sorted_index = len_argsort(valid_set_x)
        valid_set_x = [valid_set_x[i] for i in sorted_index]
        valid_set_y = [valid_set_y[i] for i in sorted_index]

        sorted_index = len_argsort(train_set_x)
        train_set_x = [train_set_x[i] for i in sorted_index]
        train_set_y = [train_set_y[i] for i in sorted_index]

    train = (train_set_x, train_set_y)
    valid = (valid_set_x, valid_set_y)
    test = (test_set_x, test_set_y)

    ####################################################################################
    # check Y_val = 1
    counter = 0
    for idx in range(len(train_set_x)):
        if train_set_y[idx] == 1:
            print "Y_val == 1"
            print train_set_x[idx]
            print train_set_y[idx]
            counter+=1
        if counter > 5:
            break
    # check Y_val = 0
    counter = 0
    for idx in range(len(train_set_x)):
        if train_set_y[idx] == 0:
            print "Y_val == 0"
            print train_set_x[idx]
            print train_set_y[idx]
            counter+=1
        if counter > 5:
            break   

    #################################################################
    return train, valid, test


'''
####################################################################################
'''

def load_data_for_prediction(dataset_name, path_dataset, 
                             n_words=60000, maxlen=None):
    '''
    Loads the dataset for prediction
    thus not test and validation set. 
    
    dataset_name: e.g. 'FullScoredTweets_train.pkl'
    path_dataset: e.g. '../Data/DataSet_Training/'
    '''
    #################################################################
    # Load the dataset
    
    root_path = os.path.dirname(os.path.realpath('__file__'))
    dataset = os.path.join(root_path, path_dataset)
    dataset = dataset + dataset_name
    
    f = open(dataset, 'rb')
    # prediction dataset is generated by 3 pickle dumps
    single_data_set = pickle.load(f)
    tweetIDs = pickle.load(f)

    print "check single_data_set: "
    print single_data_set[0][:3]
    print single_data_set[1][:3]    

    f.close()
    #################################################################   

    # limit sentence length by maxlen
    if maxlen:
        new_single_data_set_x = []
        new_single_data_set_y = []
        for x, y in zip(single_data_set[0], single_data_set[1]):
            if len(x) < maxlen:
                new_single_data_set_x.append(x)
                new_single_data_set_y.append(y)
        single_data_set = (new_single_data_set_x, new_single_data_set_y)
        del new_single_data_set_x, new_single_data_set_y

    #################################################################
    def remove_unk(x):
        # flag words by whether their frequency is above the threshold or not
        # threshod set by n_words and the dict() used to word2index 
        # NLTK tokenize: unkown word token is at the start of freq ranking, with index=0
        return [ [0 if w >= n_words else w for w in sen] for sen in x ]
    #################################################################

    single_data_set_x, single_data_set_y = single_data_set
    single_data_set_x = remove_unk(single_data_set_x)

    single_data_set = (single_data_set_x, single_data_set_y)
    
    #################################################################
    return single_data_set, tweetIDs

